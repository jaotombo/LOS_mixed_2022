{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c724d0",
   "metadata": {
    "id": "38c724d0"
   },
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HW6mjXDAOX9I",
   "metadata": {
    "id": "HW6mjXDAOX9I"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e4618",
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1668380487867,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "ae7e4618"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import dill\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "try:\n",
    "  from unidecode import unidecode\n",
    "except:\n",
    "  !pip install unidecode\n",
    "  from unidecode import unidecode\n",
    "\n",
    "pd.set_option('display.max_rows', None)  ###\n",
    "pd.set_option('display.max_columns', None)  ###\n",
    "pd.set_option('display.width', None)  ###\n",
    "pd.set_option('display.max_colwidth', None)  ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
    "except:\n",
    "    from spacy.cli import download\n",
    "\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82c396",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 209049,
     "status": "error",
     "timestamp": 1668380829040,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "3d82c396",
    "outputId": "bb1975f2-82e9-4f44-8886-0cfa94fc0479"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "if IN_COLAB:  \n",
    "  # Mount the Google Drive at mount\n",
    "  mount='/content/gdrive'\n",
    "  print(\"Colab: mounting Google drive on \", mount)\n",
    "  # connect your colab with the drive\n",
    "  drive.mount(mount)\n",
    "\n",
    " # Switch to the directory on the Google Drive that you want to use\n",
    "  import os\n",
    "  path_to_repo = mount + \"/MyDrive/MIMIC-III Text Mining/LOS_FINAL/\"\n",
    "\n",
    "else:\n",
    "  # Setup Repository\n",
    "  with open(\"repo_info.txt\", \"r\") as repo_info:\n",
    "      path_to_repo = repo_info.readline()\n",
    "\n",
    "  \n",
    "print(path_to_repo)\n",
    "\n",
    "path_to_data = f\"{path_to_repo}data/\"\n",
    "path_to_raw = f\"{path_to_data}raw/\"\n",
    "path_to_processed = f\"{path_to_data}processed/\"\n",
    "path_to_lda = f\"{path_to_data}lda/\"\n",
    "path_to_icd = f\"{path_to_data}icd_codes/\"\n",
    "path_to_models = f\"{path_to_repo}models/\"\n",
    "path_to_results = f\"{path_to_repo}results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba7f01",
   "metadata": {
    "id": "08ba7f01"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "session_seed = 42 # set seed for our session\n",
    "test_proportion = 0.2\n",
    "\n",
    "MAX_FEATURES = 10000 # maximum number of features\n",
    "min_df = 5 # minimum frequency\n",
    "max_df = 0.8 # maximum frequency\n",
    "N_GRAM = (1,2) # n_gram range\n",
    "\n",
    "seed_tag = f'_{session_seed}'\n",
    "\n",
    "random.seed(session_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ku7Szz9IQGij",
   "metadata": {
    "id": "Ku7Szz9IQGij"
   },
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7LiGsC7XoMqV",
   "metadata": {
    "id": "7LiGsC7XoMqV"
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "file = f'{path_to_processed}df_mixed_discharge.csv.gzip'\n",
    "raw_df = pd.read_csv(file, compression = 'gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8XFuBX3noXut",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1668362278145,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "8XFuBX3noXut",
    "outputId": "36444132-cf41-4257-de91-3e9fe5416bd0"
   },
   "outputs": [],
   "source": [
    "# examine the variables (names)\n",
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y2byIwcxotCi",
   "metadata": {
    "id": "y2byIwcxotCi"
   },
   "outputs": [],
   "source": [
    "# drop the variables to be exempted from the analysis and rename new dataset\n",
    "df = raw_df.drop(columns = ['hadm_id', 'subject_id','icu_los'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lYzP2kFvIoso",
   "metadata": {
    "id": "lYzP2kFvIoso"
   },
   "outputs": [],
   "source": [
    "# Transform the LOS timedelta to days (as decimal values to increase precision)\n",
    "df['los'] = pd.to_timedelta(df.los)\n",
    "df['los'] = df.los/pd.to_timedelta(1, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ADz6My05pHhS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1668362278146,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "ADz6My05pHhS",
    "outputId": "a7899ab5-aa8c-471e-f810-48bcb831755d"
   },
   "outputs": [],
   "source": [
    "# selection criterion : only patients 18 and older and with a length of stay or 1 day or greater\n",
    "df = df.loc[(df['age']>=18) & (df['los']>=1),:]\n",
    "# compare size of dataframe before and after selection\n",
    "len(raw_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nQOF0HzYqLCO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1668362278147,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "nQOF0HzYqLCO",
    "outputId": "c154c87e-5b97-43a4-de0e-1e4bf948267a"
   },
   "outputs": [],
   "source": [
    "# check proportion of missing values\n",
    "missing = pd.DataFrame(df.isna().mean(), columns = ['proportions'])\n",
    "missing.sort_values('proportions',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4C5cc715rFi2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1668362278147,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "4C5cc715rFi2",
    "outputId": "0fa3ae3d-8ed4-4495-ad02-f79cfdc504c0"
   },
   "outputs": [],
   "source": [
    "# drop variables having more than 20 % missing values\n",
    "print(f\"Variables with more than 20% Missing Values: {list(missing.loc[missing.proportions >= 0.2].index)}\")\n",
    "df = df.drop(columns=missing.loc[missing.proportions >= 0.2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rRz15TuusBd-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1668362278446,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "rRz15TuusBd-",
    "outputId": "a806dd64-8a6d-4aca-922a-a2a981c98013"
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "df = df.interpolate()\n",
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fy8UBgWYqKT0",
   "metadata": {
    "id": "fy8UBgWYqKT0"
   },
   "source": [
    "## Binarize LOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "De_p06PwwWT2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1668362279391,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "De_p06PwwWT2",
    "outputId": "50a2e75f-71cd-4d53-8630-163b47212f30"
   },
   "outputs": [],
   "source": [
    "# check distribution of outcome\n",
    "sns.catplot(kind='box', data=df, y='los');\n",
    "sns.displot(data=df, x=\"los\", bins=30, kde=True) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOUxzXAXw29V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668362279392,
     "user": {
      "displayName": "Franck Jaotombo",
      "userId": "13595873829587329663"
     },
     "user_tz": 360
    },
    "id": "bOUxzXAXw29V",
    "outputId": "ab5410f6-4c2e-479c-824f-a1e82cd0b879"
   },
   "outputs": [],
   "source": [
    "# compute Lower and Upper Fence according to Tukey's criteria\n",
    "y = df['los']\n",
    "Q1 = np.percentile(y, 25)\n",
    "Q3 = np.percentile(y, 75)\n",
    "IQR = Q3-Q1\n",
    "LF = Q1 - 1.5*IQR\n",
    "UF = Q3 + 1.5*IQR\n",
    "print(f'First quartile = {Q1:.3f}, Third Quartile = {Q3:.3f}, Interquartile Interval = {IQR:.3f}')\n",
    "print(f'Lower Fence = {LF:.3f}, Upper Fence = {UF:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UB-bnD2zxJBv",
   "metadata": {
    "id": "UB-bnD2zxJBv"
   },
   "outputs": [],
   "source": [
    "# create categorical LOS variable where prolonged LOS is any value greater than Upper Fence\n",
    "df['los_cat'] = df['los']> UF\n",
    "df = df.drop(columns=['los'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nu9HH6QmUfRR",
   "metadata": {
    "id": "Nu9HH6QmUfRR"
   },
   "source": [
    "## Heavier Preprocessing & BoW - Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l466V00LqQVs",
   "metadata": {
    "id": "l466V00LqQVs"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True) # we initialize our stemmer\n",
    "stop_words = stopwords.words('english') # nltk stopwords\n",
    "lemmatizer = WordNetLemmatizer() # lemmatizer in case we want to lemmatize instead of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXjSvXZmWskn",
   "metadata": {
    "id": "RXjSvXZmWskn"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text =  \" \".join([x for x in text.split()if x not in stop_words]) # delete stopwords from text\n",
    "    return text\n",
    "\n",
    "def heading_clean(text):\n",
    "  text = re.sub(r'\\[\\*\\*(.+?)\\*\\*\\]', \"\", text) # we take out information in brackets\n",
    "  text = re.sub(r'(Admission Date:)|(Discharge Date:)|(Date of Birth:)|(Name:)|(Unit No:)', \"\", text, flags = re.I)\n",
    "  if preproc_heavier:\n",
    "    text = re.sub(r'Sex:', \"\", text, flags = re.I)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = unidecode((text))\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    if lemmatize == True:\n",
    "        if use_spacy == True:\n",
    "            doc = nlp(text)\n",
    "            text = \" \".join([token.lemma_ for token in doc])\n",
    "        else:\n",
    "            text =  \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
    "    else:\n",
    "        text =  \" \".join([stemmer.stem(x) for x in text.split()])\n",
    "    text = remove_stopwords(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988a4d9",
   "metadata": {
    "id": "d988a4d9"
   },
   "outputs": [],
   "source": [
    "def vectorize_to_dataframe(df, vectorizer_obj):\n",
    "    \"\"\"\n",
    "    Function to return a dataframe from our vectorizer results\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data = df.toarray(), columns = vectorizer_obj.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba30881",
   "metadata": {
    "id": "5ba30881"
   },
   "outputs": [],
   "source": [
    "def vectorize_features(X_train, X_test, method = 'frequency'):\n",
    "    \"\"\"\n",
    "    Function to perform vectorization of our test sets\n",
    "    X_train, X_test, X_val: our dataframes\n",
    "    method: either 'frequency', 'tf_idf', 'onehot' to employ a different BoW technique\n",
    "    \"\"\"\n",
    "    # initialize our vectorizer\n",
    "    if method == 'tf_idf':\n",
    "        vectorizer = TfidfVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'frequency':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES)\n",
    "    elif method == 'onehot':\n",
    "        vectorizer = CountVectorizer(ngram_range=N_GRAM, min_df=min_df, max_df=max_df, max_features=MAX_FEATURES, binary = True)\n",
    "        \n",
    "    train_text = vectorizer.fit_transform(X_train.text)\n",
    "    train_text = vectorize_to_dataframe(train_text, vectorizer)\n",
    "    test_text = vectorizer.transform(X_test.text)\n",
    "    test_text = vectorize_to_dataframe(test_text, vectorizer)\n",
    "    return train_text, test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c5e17",
   "metadata": {
    "id": "be1c5e17"
   },
   "outputs": [],
   "source": [
    "def save_dataframes(train, test, method, target = False):\n",
    "    \"\"\"\n",
    "    Function to save our dataframes\n",
    "    train: train set to be saved\n",
    "    method: method through which we have processed the dataframes, needed as save keyword\n",
    "    test: test set to be saved\n",
    "    \"\"\"\n",
    "    # need to reset the index\n",
    "    train.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    train.to_feather('{}{}train_{}{}{}'.format(path_to_processed, method, seed_tag, preproc_tag, lemma_tag))\n",
    "    # need to reset the index\n",
    "    test.reset_index(inplace=True, drop = True)\n",
    "    # save our dataset up to now in feather format\n",
    "    test.to_feather('{}{}test_{}{}{}'.format(path_to_processed, method, seed_tag, preproc_tag, lemma_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(method):\n",
    "    \"\"\"\n",
    "    Function to load train, test and validation set based on the chosen method\n",
    "    method: string for the processing method we want to load\n",
    "    \"\"\"\n",
    "    global path_to_processed\n",
    "    # load it back\n",
    "    train = pd.read_feather('{}{}train_{}{}{}'.format(path_to_processed, method, seed_tag, preproc_tag, lemma_tag))\n",
    "    test = pd.read_feather('{}{}test_{}{}{}'.format(path_to_processed, method, seed_tag, preproc_tag, lemma_tag))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B8hNsOl6gEt4",
   "metadata": {
    "id": "B8hNsOl6gEt4"
   },
   "source": [
    "### Vectorization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z4mZ0EbwdkAP",
   "metadata": {
    "id": "z4mZ0EbwdkAP"
   },
   "outputs": [],
   "source": [
    "# Iterate over our main methods of vectorization\n",
    "\n",
    "vect_dict = {'stemming': (False, False),\n",
    "             'spacy': (True, True)}\n",
    "\n",
    "             \n",
    "for key, value in vect_dict.items():\n",
    "    print(key)\n",
    "    # PARAMETERS\n",
    "\n",
    "    lemmatize = value[0] # set to false if we want to do stemming\n",
    "    lemma_tag = str(np.where(lemmatize, \"_lemma\",\"\"))\n",
    "    use_spacy = value[1]\n",
    "    if use_spacy: lemma_tag = str(np.where(lemmatize, \"_lemma_spacy\",\"\"))\n",
    "\n",
    "    preprocessing = True # set to true if we want to clean and perform some preprocessing\n",
    "    preproc_heavier = True # set to True if we want a heavier preprocessing\n",
    "    preproc_tag_2 = np.where(preproc_heavier, '_heavier', '')\n",
    "    preproc_tag = np.where(preprocessing, f'_preproc{preproc_tag_2}', f'{preproc_tag_2}')\n",
    "  \n",
    "    if preprocessing:\n",
    "        try:\n",
    "            df = pd.read_feather(f'{path_to_processed}df_los{preproc_tag}{lemma_tag}')\n",
    "            print('Dataframe Loaded')\n",
    "        except:\n",
    "            df['text'] = df.text.apply(lambda x: heading_clean(x))\n",
    "            # need to reset the index\n",
    "            df.reset_index(inplace=True, drop = True)\n",
    "            # save our dataset up to now in feather format\n",
    "            df.to_feather(f'{path_to_processed}df_los{preproc_tag}{lemma_tag}')\n",
    "            print('Dataframe Saved')\n",
    "        # split the data into training and test\n",
    "        train, test = train_test_split(df, train_size=0.80, stratify = df['los_cat'], random_state=42)\n",
    "    # perform vectorization\n",
    "    method_list = ['frequency', 'onehot','tf_idf']\n",
    "    train_clean = train.drop(columns = 'text')\n",
    "    train_clean.reset_index(inplace = True, drop = True)\n",
    "    test_clean = test.drop(columns = 'text')\n",
    "    test_clean.reset_index(inplace = True, drop = True)\n",
    "    for method in method_list:\n",
    "        print(method)\n",
    "        # for each method we perform vectorization\n",
    "        train_text, test_text = vectorize_features(train, test, method = method)\n",
    "        # Drop the text column and concatenate it\n",
    "        X_train = pd.concat([train_clean, train_text], axis = 1)\n",
    "        X_test = pd.concat([test_clean, test_text], axis = 1)\n",
    "        assert X_train.shape[0] == train_clean.shape[0]\n",
    "        assert X_test.shape[0] == test_clean.shape[0]\n",
    "        # and save the dataframes\n",
    "        save_dataframes(X_train, X_test, method = method)\n",
    "        # And also perform LDA\n",
    "        try:\n",
    "            with open(f'{path_to_lda}lda{seed_tag}{method}{preproc_tag}{lemma_tag}', 'rb') as file: # and save the fitted model\n",
    "                lda = dill.load(file)\n",
    "            train_lda = lda.transform(train_text)\n",
    "        except:\n",
    "            lda = LatentDirichletAllocation(n_components = 300, random_state = session_seed, n_jobs = -1)\n",
    "            train_lda = lda.fit_transform(train_text)\n",
    "        train_lda = pd.DataFrame(train_lda)\n",
    "        train_lda.columns=[\"F\"+str(i) for i in range(0, len(train_lda.columns))]\n",
    "        test_lda = lda.transform(test_text)\n",
    "        test_lda = pd.DataFrame(test_lda)\n",
    "        test_lda.columns=[\"F\"+str(i) for i in range(0, len(test_lda.columns))]\n",
    "        # Drop the text column and concatenate it\n",
    "        X_train = pd.concat([train_clean, train_lda], axis = 1)\n",
    "        X_test = pd.concat([test_clean, test_lda], axis = 1)\n",
    "        assert X_train.shape[0] == train_clean.shape[0]\n",
    "        assert X_test.shape[0] == test_clean.shape[0]\n",
    "        save_dataframes(X_train, X_test, method = f'lda_{method}')\n",
    "        with open(f'{path_to_lda}lda{seed_tag}{method}{preproc_tag}{lemma_tag}', 'wb') as file: # and save the fitted model\n",
    "            dill.dump(lda, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1GLXFeakPMXc3JApHA7uOYi63SwnVV4oE",
     "timestamp": 1668380415992
    },
    {
     "file_id": "https://github.com/jaotombo/mimim_iii_readmission/blob/main/notebooks/3.%20Text%20Vectorization.ipynb",
     "timestamp": 1657015451471
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fc10b2d10f9f16d25710b4d4512c52b634d4a9283a799a319e31d4f4e23ed6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
